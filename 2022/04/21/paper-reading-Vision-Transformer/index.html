

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="/img/favicon.ico">
  <link rel="icon" href="/img/favicon.ico">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
  <meta name="theme-color" content="#84674f">
  <meta name="author" content="Ryan LI">
  <meta name="keywords" content="">
  
    <meta name="description" content="Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks suc">
<meta property="og:type" content="article">
<meta property="og:title" content="Vision Transformer">
<meta property="og:url" content="https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/index.html">
<meta property="og:site_name" content="ShouRou">
<meta property="og:description" content="Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks suc">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://daydreamatnight.github.io/index/paper-reading-ViT.png">
<meta property="article:published_time" content="2022-04-21T12:42:10.000Z">
<meta property="article:modified_time" content="2022-06-09T10:25:25.381Z">
<meta property="article:author" content="Ryan LI">
<meta property="article:tag" content="deep learning">
<meta property="article:tag" content="paper reading">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://daydreamatnight.github.io/index/paper-reading-ViT.png">
  
  
    <meta name="referrer" content="no-referrer-when-downgrade">
  
  
  <title>Vision Transformer - ShouRou</title>

  <link  rel="stylesheet" href="https://lib.baomitu.com/twitter-bootstrap/4.6.1/css/bootstrap.min.css" />



  <link  rel="stylesheet" href="https://lib.baomitu.com/github-markdown-css/4.0.0/github-markdown.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/hint.css/2.7.0/hint.min.css" />

  <link  rel="stylesheet" href="https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.css" />



<!-- 主题依赖的图标库，不要自行修改 -->
<!-- Do not modify the link that theme dependent icons -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_hj8rtnfg7um.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />


  <link id="highlight-css" rel="stylesheet" href="/css/highlight.css" />
  
    <link id="highlight-css-dark" rel="stylesheet" href="/css/highlight-dark.css" />
  




  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    Fluid.ctx = Object.assign({}, Fluid.ctx)
    var CONFIG = {"hostname":"daydreamatnight.github.io","root":"/","version":"1.9.3","typing":{"enable":true,"typeSpeed":1,"cursorChar":"","loop":false,"scope":[]},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"left","visible":"hover","icon":"§"},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"code_language":{"enable":true,"default":"TEXT"},"copy_btn":true,"image_caption":{"enable":true},"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"placement":"right","headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":4},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":true,"offset_factor":2},"web_analytics":{"enable":false,"follow_dnt":true,"baidu":null,"google":null,"gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};

    if (CONFIG.web_analytics.follow_dnt) {
      var dntVal = navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack;
      Fluid.ctx.dnt = dntVal && (dntVal.startsWith('1') || dntVal.startsWith('yes') || dntVal.startsWith('on'));
    }
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
  


  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.0.0"></head>


<body>
  

  <header>
    

<div class="header-inner" style="height: 40vh;">
  <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>ShouRou</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
          
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

  

<div id="banner" class="banner" parallax=true
     style="background: url('/img/marble1.jpg') no-repeat center center; background-size: cover;">
  <div class="full-bg-img">
    <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.6)">
      <div class="banner-text text-center fade-in-up">
        <div class="h2">
          
            <span id="subtitle" data-typed-text="Vision Transformer"></span>
          
        </div>

        
          
  <div class="mt-3">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-author" aria-hidden="true"></i>
        Ryan LI
      </span>
    
    
      <span class="post-meta">
        <i class="iconfont icon-date-fill" aria-hidden="true"></i>
        <time datetime="2022-04-21 20:42" pubdate>
          April 21, 2022 pm
        </time>
      </span>
    
  </div>

  <div class="mt-1">
    
      <span class="post-meta mr-2">
        <i class="iconfont icon-chart"></i>
        
          <!-- compatible with older versions-->
          12k words
        
      </span>
    

    
      <span class="post-meta mr-2">
        <i class="iconfont icon-clock-fill"></i>
        
        
        
          <!-- compatible with older versions-->
          41 minutes
        
      </span>
    

    
    
  </div>


        
      </div>

      
    </div>
  </div>
</div>

</div>

  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="side-col d-none d-lg-block col-lg-2">
      

    </div>

    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">Vision Transformer</h1>
            
            
              <div class="markdown-body">
                
                <blockquote>
<p>Presented in 2021, the vision transformer model (ViT) is the most influential work in the CV field recent years. Its variants outperform the dominant convolutional networks in almost all CV tasks such as <a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/image-classification-on-imagenet">classification</a> and <a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/object-detection-on-coco">object detection</a>. And it breaks the border of CV and NLP, providing new thoughts to CV and multi-model areas.</p>
</blockquote>
<blockquote>
<p>This is a <a href="/2022/04/02/paper-reading-start/">series of paper reading notes</a>, hopefully, to push me to read paper casually and to leave some record of what I've learned.</p>
</blockquote>
<span id="more"></span>
<p>Paper:</p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929v2">An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</a></p>
<p>Useful links:</p>
<p>https://www.bilibili.com/video/BV15P4y137jb</p>
<p>https://theaisummer.com/vision-transformer/</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/VIT properties.png" srcset="/img/loading.gif" lazyload alt="VIT properties" style="zoom:50%;"></p>
<p>Not only Vision Transformer (ViT) performs better on traditional CV tasks, it has more impressive properties. As shown above, <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html">Naseer et al.</a> demonstrate the tasks where ViT shows extra performance over CNN models, even over humans.</p>
<h2 id="notes">Notes</h2>
<h3 id="abstract">Abstract</h3>
<p>While Transformer-based models such as BERT, GPT series, and T5 nail the NLP tasks, CV tasks remain dominated by CNN-based models. This paper applied a pure transformer encoder (same as BERT) to sequences of cut images and obtains a good classification result, especially with <strong>supervised</strong> pre-training on a large dataset then fine tuning on a mid-size dataset. Besides, fewer computational resources (meaning 2500 days of TPUv3) are need to attain good results, compared with CNN models.</p>
<h3 id="conclusion">Conclusion</h3>
<p>Besides the paraphrasing part, the conclusion part discusses the future work based on the ViT. And all of them have follow-up works.</p>
<ul>
<li>Apply ViT to other CV tasks, given the promising performance of <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.12872.pdf,">DETR</a>. Only 1 and a half month later, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09958">ViT-FRCNN</a> and <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html">SEDR</a> mange to apply ViT on detection and segmentation respectively. And after 3 months, <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html">Swin Transformer</a> introduces hierarchical feature to transformer, making ViT more suitable to vision tasks.</li>
<li>Self-supervised pre-training, given the great results of BERT and GPT in the NLP field. Initial explorations in the paper show a gap from the supervised pre-training. One year later, <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">MAE</a> narrows the gap successfully by generative model.
<ul>
<li>Besides, in the section of self-supervised learning, a contrastive learning is mentioned as well, and <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html">MOCO v3</a> and <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html">DINO</a> follow this line.</li>
</ul></li>
<li>Further scaling up this model. Half year later, same group introduces <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2106.04560v1.pdf">Vit-G</a> with two billion parameters, attaining new SOTA on ImageNet of 90.45%.</li>
</ul>
<h3 id="introduction">Introduction</h3>
<p>Success of Transformer-based models on NLP tasks are firstly reviewed. And it is natural trying to apply such self-attention mechanism to vision. Yet here are one major obstacle:</p>
<ul>
<li>How to transfer a 2D picture to a 1D sequence?</li>
</ul>
<p>One intuitive thought is to flatten the picture directly and treat each pixel as an element. In this way, a medium size 224*224 picture will be converted to a 50,176 long sequence. However, the sequence length is quadratically related to model complexity. Morden hardware only supports input sequence length &lt;1000 of a pure self-attention model. For example BERT only accepts input length of 512.</p>
<p>Yet the authors mange to incorporate the original transformer encoder in CV. In order to address the sequence length problem, they split the image into 16*16 patches, each patches denotes a sequence element (token). In this way, a 224*224 image can be converted as a sequence length of 16*16 with each element sized 14*14. Each element then gets linearised through a FC layer before being passed into the transformer encoder.</p>
<p>And the afterwards experiments show that the new model doesn't perform well on mid-size model. One explanation is that transformer model lack the image-related inductive bias of CNN (locality and translation equalisation). Yet with pre-training on large dataset such as JFT-300 and ImagNet-21K, a better result than CNN can be approached.</p>
<h3 id="related-work">Related work</h3>
<p><em>It is a detailed related work covering all the aspects in the original paper. I just pick few of them.</em></p>
<p>All the related works aim at reducing the sequence length within the limitation caused by self-attention. Some try to combine CNN with self-attention. For example <a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html">Wang, X et al.'s work</a> takes the feature map extracted by CNN as the input of transformer. Others try to replace the CNN with a special variation of self-attention. For example, <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html">Ramachandran et al.'s work</a> replaces all convolutional sublayers of the ResNet-50 model with self-attention layers. To reduce the computational cost, a local region of the image instead of the whole image is used as the receptive field of the self-attention layer, meaning each pixel only attends to its neighbours in a restricted area. In another work line, <a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7">Wang, H et.al's work</a> factorising 2D self-attention into two 1D self-attentions to significantly reduce computation complexity.</p>
<p>And there is a very similar <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.03584">Cordonnier et al's work</a> also split the images before the self-attention layer. Yet the patch size is 2*2 and with the dataset only <a target="_blank" rel="noopener" href="https://paperswithcode.com/sota/image-classification-on-cifar-10">CIFAR-10</a>. This paper enlarges the model, apply it in big dataset, and shows the scalability.</p>
<p>Another related work image GPT(<a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/chen20s.html">iGPT</a>) trains a GPT-2 scale generative network. Yet the highest accuracy on ImageNet is 72%, way less than 88% of this paper. But in 2021, an afterwards generative network <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">MAE</a> shows a competitive result of 87.8%, with good transfer leaning capability on segmentation and object detection as well.</p>
<p>Besides, works exploring transfer learning performance of CNN model on larger datasets such as ImageNet-21k and JFT-300M are mentioned. And this paper studies the transformer instead of the CNN.</p>
<h3 id="method">Method</h3>
<p>The whole big idea of the method part is leaving as much as possible the original transformer architecture in order to leverage the good feature and the existing mature efficient implementations of it.</p>
<h4 id="vision-transformer-vit">Vision Transformer (VIT)</h4>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/VIT model overview.png" srcset="/img/loading.gif" lazyload alt="VIT model overview" style="zoom:60%;"></p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/VIT algorithm.png" srcset="/img/loading.gif" lazyload alt="VIT algorithm" style="zoom:40%;"></p>
<p>From the overview, and the algorithm it should be called Vision BERT instead of Vision Transformer, given the pure encoder architecture and the extra "classification token". Assume a <em>224*224*3</em> image, after patching, the sequence length is <em>HW/P<sup>2</sup>=16*16=196</em> and the width is <em>14*14*3=768</em>. Given the hidden size of the model <em>D=768</em>, through linear projection layer (E), sequence <em>X [196*768]</em> are multiplied with weight <em>E [768*768]</em>. The resulting linear output <em>[196*768]</em> is then contacted with [cls], followed by adding standard 1D learnable positional embedding to be the transformer input <em>[197*768]</em>. After several transformer blocks, the output size does not change and the [cls] token is projected to a softmax classification layer</p>
<h4 id="ablation-experiments">Ablation experiments</h4>
<p>The pre and post-processing are crucial for ViT given that the middle transformer encoder layers are kept as original. Multiple rounds of ablation experiments are carried out.</p>
<ul>
<li><p>Position embedding schemes, 2D embedding and relative embedding are applied to compare with the standard 1D embedding, and no evident gain is spotted.</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/VIT positional embedding ablation.png" srcset="/img/loading.gif" lazyload alt="VIT positional embedding ablation" style="zoom:30%;"></p></li>
<li><p>[cls] token vs average pooling, extra [cls] token is inherited from the Transformer model for text, and traditionally in CV, instead of an additional token, an average polling layer after the output layer is usually used as a classifier. The figure blow shows no both works. But in order to stick the original design as close as possible, [cls] token is applied.</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/VIT class token ablation.png" srcset="/img/loading.gif" lazyload alt="VIT class token ablation" style="zoom:30%;"></p></li>
</ul>
<p>Another analysis after the model description:</p>
<p><strong>Inductive bias:</strong> Less locality, translation equivariance and 2D neighbourhood structure are possessed by ViT, compared with CNN.</p>
<p><strong>Hybrid Architecture:</strong> CNN can be used as a special embedding, leveraging the inductive bias of the CNN model.</p>
<h4 id="limitation-on-fine-tuning">Limitation on Fine-Tuning</h4>
<p>Pre-train ViT at larger and higher resolution datasets is <a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html">proved</a> to be beneficial. Yet the input sequence lengths are different when training on two datasets with different resolutions, resulting in positional embeddings of different lengths. In this article, a 2D interpolation is applied to transfer a pre-trained positional embedding to another dataset to fine-tune. But the accuracy will loss if the resolution difference is too big.</p>
<h3 id="experiments">Experiments</h3>
<h4 id="setup">Setup</h4>
<p>ResNet, ViT, and the hybrid model are evaluated together and ViT wins taking account of the pre-training cost. Besides, a small self-supervision experiment is deployed and sees potential.</p>
<p>Two scales of ImageNet(1k and 21k) and JFT(303M) are used as pre-training dataset. Only classification tasks are evaluated with popular datasets.</p>
<p>3 scales of ViT are designed with different patch size(inversely proportional to the amount of data). For example, ViT-L/16 means ViT-Large with 16 patches.</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/ViT variants.png" srcset="/img/loading.gif" lazyload alt="ViT variants" style="zoom:50%;"></p>
<h4 id="comparison-to-sota">Comparison to SOTA</h4>
<p>And the best results are shown below:</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/ViT results.png" srcset="/img/loading.gif" lazyload alt="ViT results" style="zoom:60%;"></p>
<h4 id="pre-training-cost-requirements">Pre-training cost requirements</h4>
<p>Figure 3 and 4 shows the performance of the presented models on different sizes of per-training datasets. ViT preforms competitive only starts from dataset 21k, and very well only on huge dataset.</p>
<p>Figure 5 shows the transfer performances versus pre-training costs on JFT-300M of several models. And it shows that ViT is cheaper than ResNet. Interestingly, the Hybrid model is competitive on low pre-training cost.</p>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/ViT ablation 2.png" srcset="/img/loading.gif" lazyload alt="ViT ablation 2" style="zoom:55%;"></p>
<h4 id="inspecting-vit">Inspecting ViT</h4>
<p><img src="/2022/04/21/paper-reading-Vision-Transformer/ViT inspecting.png" srcset="/img/loading.gif" lazyload alt="ViT inspecting" style="zoom:42%;"></p>
<p><strong>Figure7 Left:</strong> The learned linear projection weight matrix <em>E [768*768]</em> is inspected by PCA, and the first 28 components(modes) are visualised as embedding filters. They look pretty much similar to the early layer filters(kernels) of CNN (for example, the first layer of a CNN shown by <a target="_blank" rel="noopener" href="https://www.mdpi.com/2073-8994/8/12/144">Brachnmann et.al</a>). This similarity indicates that the linear patch embedding manages to represent the low-dimension structure of each patch. <img src="/2022/04/21/paper-reading-Vision-Transformer/CNN first layer filters.png" srcset="/img/loading.gif" lazyload alt="CNN first layer filters" style="zoom:40%;"></p>
<p><strong>Figure7 mid:</strong> Position embedding visualisation first shows that the spatial information is captured well by the E matrix, given that the similarity matrix between patches matches well with the distance relationships of patches. Second, patterns across rows (and columns) have similar representations, indicating the embedding layer has successful learned the row-column relationship. Overall, the 1D positional embedding has learned the 2D structure, coherent with the ablation experiment result.</p>
<p><strong>Figure7 Right:</strong> The receptive fields of the multi-head attention layers are evaluated by the mean attention distance. Compared with CNN whose receptive field increases linearly with the depth, the ViT attends the whole picture from the first layer, leveraging the natural advantage of transformer.</p>
<h4 id="self-supervision">Self-supervision</h4>
<p>A preliminary exploration on masked patch prediction for self-supervision (mimicking one of the BERT pre-training tasks) has been employed. But the result is not satisfying. And contrastive pre-training are mentioned as a future work.</p>
<h2 id="review">Review</h2>
<p>This is a concise-written, fundamental article. Just as presented in the conclusion part, it inspired many flow-up works from any direction in the CV area, such as applying to more tasks, changing the architecture(tokenisation, transformer block(<a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html">MLP-mixer</a>： changing multi-head attention layers to MLP , <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.11418">meta-former</a>: substituting multi-head attention layers to average pooling), changing objective function(self-supervised, contrastive learning), and multi modality.</p>
<p>It's still unclear whether convolution, attention or MLP will win this game.</p>
<h2 id="reference">Reference</h2>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/c404a5adbf90e09631678b13b05d9d7a-Abstract.html">Naseer, M. M., Ranasinghe, K., Khan, S. H., Hayat, M., Shahbaz Khan, F., &amp; Yang, M. H. (2021). Intriguing properties of vision transformers. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09958">Beal, J., Kim, E., Tzeng, E., Park, D. H., Zhai, A., &amp; Kislyuk, D. (2020). Toward transformer-based object detection. <em>arXiv preprint arXiv:2012.09958</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/CVPR2021/html/Zheng_Rethinking_Semantic_Segmentation_From_a_Sequence-to-Sequence_Perspective_With_Transformers_CVPR_2021_paper.html">Zheng, S., Lu, J., Zhao, H., Zhu, X., Luo, Z., Wang, Y., ... &amp; Zhang, L. (2021). Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 6881-6890).</a></p>
<p><a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper.html">Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... &amp; Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 10012-10022).</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2005.12872.pdf,">Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., &amp; Zagoruyko, S. (2020, August). End-to-end object detection with transformers. In <em>European conference on computer vision</em> (pp. 213-229). Springer, Cham.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.06377">He, K., Chen, X., Xie, S., Li, Y., Dollár, P., &amp; Girshick, R. (2021). Masked autoencoders are scalable vision learners. <em>arXiv preprint arXiv:2111.06377</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Chen_An_Empirical_Study_of_Training_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html">Chen, X., Xie, S., &amp; He, K. (2021). An empirical study of training self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9640-9649).</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content/ICCV2021/html/Caron_Emerging_Properties_in_Self-Supervised_Vision_Transformers_ICCV_2021_paper.html">Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., &amp; Joulin, A. (2021). Emerging properties in self-supervised vision transformers. In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision</em> (pp. 9650-9660).</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.04560">Zhai, X., Kolesnikov, A., Houlsby, N., &amp; Beyer, L. (2021). Scaling Vision Transformers. <em>ArXiv, abs/2106.04560</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://openaccess.thecvf.com/content_cvpr_2018/html/Wang_Non-Local_Neural_Networks_CVPR_2018_paper.html">Wang, X., Girshick, R., Gupta, A., &amp; He, K. (2018). Non-local neural networks. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 7794-7803).</a></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/3416a75f4cea9109507cacd8e2f2aefc-Abstract.html">Ramachandran, P., Parmar, N., Vaswani, A., Bello, I., Levskaya, A., &amp; Shlens, J. (2019). Stand-alone self-attention in vision models. <em>Advances in Neural Information Processing Systems</em>, <em>32</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://link.springer.com/chapter/10.1007/978-3-030-58548-8_7">Wang, H., Zhu, Y., Green, B., Adam, H., Yuille, A., &amp; Chen, L. C. (2020, August). Axial-deeplab: Stand-alone axial-attention for panoptic segmentation. In <em>European Conference on Computer Vision</em> (pp. 108-126). Springer, Cham.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1911.03584">Cordonnier, J. B., Loukas, A., &amp; Jaggi, M. (2019). On the relationship between self-attention and convolutional layers. <em>arXiv preprint arXiv:1911.03584</em>.</a></p>
<p><a target="_blank" rel="noopener" href="http://proceedings.mlr.press/v119/chen20s.html">Chen, Mark, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. "Generative pretraining from pixels." In <em>International Conference on Machine Learning</em>, pp. 1691-1703. PMLR, 2020.</a></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2019/hash/d03a857a23b5285736c4d55e0bb067c8-Abstract.html">Touvron, H., Vedaldi, A., Douze, M., &amp; Jégou, H. (2019). Fixing the train-test resolution discrepancy. <em>Advances in neural information processing systems</em>, <em>32</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://www.mdpi.com/2073-8994/8/12/144">Brachmann, A., &amp; Redies, C. (2016). Using convolutional neural network filters to measure left-right mirror symmetry in images. <em>Symmetry</em>, <em>8</em>(12), 144.</a></p>
<p><a target="_blank" rel="noopener" href="https://proceedings.neurips.cc/paper/2021/hash/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Abstract.html">Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L., Zhai, X., Unterthiner, T., ... &amp; Dosovitskiy, A. (2021). Mlp-mixer: An all-mlp architecture for vision. <em>Advances in Neural Information Processing Systems</em>, <em>34</em>.</a></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2111.11418">Yu, W., Luo, M., Zhou, P., Si, C., Zhou, Y., Wang, X., ... &amp; Yan, S. (2021). Metaformer is actually what you need for vision. arXiv preprint arXiv:2111.11418.</a></p>

                
              </div>
            
            <hr/>
            <div>
              <div class="post-metas my-3">
  
  
    <div class="post-meta">
      <i class="iconfont icon-tags"></i>
      
        <a href="/tags/deep-learning/">#deep learning</a>
      
        <a href="/tags/paper-reading/">#paper reading</a>
      
    </div>
  
</div>


              
  

  <div class="license-box my-3">
    <div class="license-title">
      <div>Vision Transformer</div>
      <div>https://daydreamatnight.github.io/2022/04/21/paper-reading-Vision-Transformer/</div>
    </div>
    <div class="license-meta">
      
        <div class="license-meta-item">
          <div>Author</div>
          <div>Ryan LI</div>
        </div>
      
      
        <div class="license-meta-item license-meta-date">
          <div>Posted on</div>
          <div>April 21, 2022</div>
        </div>
      
      
      
        <div class="license-meta-item">
          <div>Licensed under</div>
          <div>
            
              
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="BY - Attribution">
                    <i class="iconfont icon-by"></i>
                  </span>
                </a>
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="NC - Non-commercial">
                    <i class="iconfont icon-nc"></i>
                  </span>
                </a>
              
                <a target="_blank" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">
                  <span class="hint--top hint--rounded" aria-label="SA - Share-alike">
                    <i class="iconfont icon-sa"></i>
                  </span>
                </a>
              
            
          </div>
        </div>
      
    </div>
    <div class="license-icon iconfont"></div>
  </div>



              
                <div class="post-prevnext my-3">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/2022/04/27/paper-reading-MAE/" title="Masked Autoencoder(MAE)">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">Masked Autoencoder(MAE)</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/2022/04/18/paper-reading-GPT1-3/" title="GPT1-3">
                        <span class="hidden-mobile">GPT1-3</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>

    <div class="side-col d-none d-lg-block col-lg-2">
      
  <aside class="sidebar" style="margin-left: -1rem">
    <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;Table of Contents</p>
  <div class="toc-body" id="toc-body"></div>
</div>



  </aside>


    </div>
  </div>
</div>





  



  



  



  



  


  
  





  <script>
  Fluid.utils.createScript('https://lib.baomitu.com/mermaid/8.14.0/mermaid.min.js', function() {
    mermaid.initialize({"theme":"default"});

    Fluid.events.registerRefreshCallback(function() {
      if ('mermaid' in window) {
        mermaid.init();
      }
    });
  });
</script>






    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v" for="local-search-input">Keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>

    

    
  </main>

  <footer>
    <div class="footer-inner">
  
    <div class="footer-content">
       <a href="https://lsongrui.github.io/" target="_blank" rel="nofollow noopener"><span>Shoushou</span></a> <i class="iconfont icon-love"></i> <a href="https://jingyicc.github.io/" target="_blank" rel="nofollow noopener"><span>Rourou</span></a> 
    </div>
  
  
    <div class="statistics">
  
  

  
    
      <span id="busuanzi_container_site_pv" style="display: none">
        Toal views: 
        <span id="busuanzi_value_site_pv"></span>
         
      </span>
    
    
      <span id="busuanzi_container_site_uv" style="display: none">
        Total visiters: 
        <span id="busuanzi_value_site_uv"></span>
        
      </span>
    
    
  
</div>

  
  
  
</div>

  </footer>

  <!-- Scripts -->
  
  <script  src="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://lib.baomitu.com/nprogress/0.2.0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://lib.baomitu.com/jquery/3.6.0/jquery.min.js" ></script>
<script  src="https://lib.baomitu.com/twitter-bootstrap/4.6.1/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>


  <script  src="https://lib.baomitu.com/typed.js/2.0.12/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var subtitle = document.getElementById('subtitle');
      if (!subtitle || !typing) {
        return;
      }
      var text = subtitle.getAttribute('data-typed-text');
      
        typing(text);
      
    })(window, document);
  </script>




  
    
      <script  src="/js/img-lazyload.js" ></script>
    
  




  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/tocbot/4.18.2/tocbot.min.js', function() {
    var toc = jQuery('#toc');
    if (toc.length === 0 || !window.tocbot) { return; }
    var boardCtn = jQuery('#board-ctn');
    var boardTop = boardCtn.offset().top;

    window.tocbot.init(Object.assign({
      tocSelector     : '#toc-body',
      contentSelector : '.markdown-body',
      linkClass       : 'tocbot-link',
      activeLinkClass : 'tocbot-active-link',
      listClass       : 'tocbot-list',
      isCollapsedClass: 'tocbot-is-collapsed',
      collapsibleClass: 'tocbot-is-collapsible',
      scrollSmooth    : true,
      includeTitleTags: true,
      headingsOffset  : -boardTop,
    }, CONFIG.toc));
    if (toc.find('.toc-list-item').length > 0) {
      toc.css('visibility', 'visible');
    }

    Fluid.events.registerRefreshCallback(function() {
      if ('tocbot' in window) {
        tocbot.refresh();
        var toc = jQuery('#toc');
        if (toc.length === 0 || !tocbot) {
          return;
        }
        if (toc.find('.toc-list-item').length > 0) {
          toc.css('visibility', 'visible');
        }
      }
    });
  });
</script>


  <script src=https://lib.baomitu.com/clipboard.js/2.0.11/clipboard.min.js></script>

  <script>Fluid.plugins.codeWidget();</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/anchor-js/4.3.1/anchor.min.js', function() {
    window.anchors.options = {
      placement: CONFIG.anchorjs.placement,
      visible  : CONFIG.anchorjs.visible
    };
    if (CONFIG.anchorjs.icon) {
      window.anchors.options.icon = CONFIG.anchorjs.icon;
    }
    var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
    var res = [];
    for (var item of el) {
      res.push('.markdown-body > ' + item.trim());
    }
    if (CONFIG.anchorjs.placement === 'left') {
      window.anchors.options.class = 'anchorjs-link-left';
    }
    window.anchors.add(res.join(', '));

    Fluid.events.registerRefreshCallback(function() {
      if ('anchors' in window) {
        anchors.removeAll();
        var el = (CONFIG.anchorjs.element || 'h1,h2,h3,h4,h5,h6').split(',');
        var res = [];
        for (var item of el) {
          res.push('.markdown-body > ' + item.trim());
        }
        if (CONFIG.anchorjs.placement === 'left') {
          anchors.options.class = 'anchorjs-link-left';
        }
        anchors.add(res.join(', '));
      }
    });
  });
</script>


  
<script>
  Fluid.utils.createScript('https://lib.baomitu.com/fancybox/3.5.7/jquery.fancybox.min.js', function() {
    Fluid.plugins.fancyBox();
  });
</script>


  <script>Fluid.plugins.imageCaption();</script>

  
      <script>
        if (!window.MathJax) {
          window.MathJax = {
            tex    : {
              inlineMath: { '[+]': [['$', '$']] }
            },
            loader : {
              load: ['ui/lazy']
            },
            options: {
              renderActions: {
                insertedScript: [200, () => {
                  document.querySelectorAll('mjx-container').forEach(node => {
                    let target = node.parentNode;
                    if (target.nodeName.toLowerCase() === 'li') {
                      target.parentNode.classList.add('has-jax');
                    }
                  });
                }, '', false]
              }
            }
          };
        } else {
          MathJax.startup.document.state(0);
          MathJax.texReset();
          MathJax.typeset();
          MathJax.typesetPromise();
        }

        Fluid.events.registerRefreshCallback(function() {
          if ('MathJax' in window && MathJax.startup.document && typeof MathJax.startup.document.state === 'function') {
            MathJax.startup.document.state(0);
            MathJax.texReset();
            MathJax.typeset();
            MathJax.typesetPromise();
          }
        });
      </script>
    

  <script  src="https://lib.baomitu.com/mathjax/3.2.2/es5/tex-mml-chtml.js" ></script>

  <script  src="/js/local-search.js" ></script>

  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  
<script src="//cdn.jsdelivr.net/gh/EmoryHuang/BlogBeautify@1.1/DynamicLine.min.js"></script>
<script src="//cdn.jsdelivr.net/npm/echarts@4.8.0/dist/echarts.min.js".js"></script>
<script src="/%3Cscript%20src=%22https:/cdn.jsdelivr.net/npm/echarts-gl@1.1.1/dist/echarts-gl.min.js"></script>



<!-- 主题的启动项，将它保持在最底部 -->
<!-- the boot of the theme, keep it at the bottom -->
<script  src="/js/boot.js" ></script>


  

  <noscript>
    <div class="noscript-warning">Blog works best with JavaScript enabled</div>
  </noscript>
</body>
</html>
